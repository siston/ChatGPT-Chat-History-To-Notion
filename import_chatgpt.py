# -*- coding: utf-8 -*-
"""
ChatGPT Chat History To Notion

æç®€ä½¿ç”¨æ–¹æ³•:
1. pip install requests tqdm
2. åœ¨è„šæœ¬é¡¶éƒ¨çš„é…ç½®åŒºåŸŸå¡«å…¥ä½ çš„ API å¯†é’¥å’Œæ•°æ®åº“ ID
3. python import_chatgpt_fixed.py

è¯¦ç»†æ–‡æ¡£ï¼šhttps://github.com/Pls-1q43/ChatGPT-Full-Log-To-Notion/
"""

import json
import requests
import datetime
import time
import os
import mimetypes
import sys
import tempfile
from tqdm import tqdm
import re

# --- é…ç½®åŒºåŸŸ ---
# è¯·åœ¨ä¸‹æ–¹å¡«å…¥ä½ çš„é…ç½®ä¿¡æ¯

# 1. ä½ çš„ Notion Integration Token (API å¯†é’¥)
# è·å–æ–¹å¼: https://www.notion.so/my-integrations ï¼ˆä¸€ä¸²ä»¥ ntn_ å¼€å¤´çš„å­—ç¬¦ä¸²ï¼‰
NOTION_API_KEY = ""

# 2. ä½ çš„ Notion æ•°æ®åº“ ID  
# è·å–æ–¹å¼: ä»æ•°æ®åº“URLä¸­å¤åˆ¶ï¼ˆæ¯”å¦‚ï¼ŒURLä¸ºï¼šhttps://www.notion.so/223ca795c956806f84b8da595d3647d6ï¼Œåˆ™å¡«å†™223ca795c956806f84b8da595d3647d6ï¼‰
NOTION_DATABASE_ID = ""

# 3. ChatGPT å¯¼å‡ºæ–‡ä»¶å¤¹è·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•)
CHATGPT_EXPORT_PATH = "./"

# === æ–°å¢ï¼šå›¾ç‰‡è°ƒè¯•å¼€å…³ ===
DEBUG_IMAGE_UPLOAD = False  # è®¾ç½®ä¸º True æˆ–é€šè¿‡ç¯å¢ƒå˜é‡ DEBUG_IMAGE_UPLOAD=1 å¼€å¯

# === æ–°å¢ï¼šå¿«é€Ÿæµ‹è¯•æ¨¡å¼å¼€å…³ ===
QUICK_TEST_MODE = False  # å…¨é‡å¯¼å…¥æ¨¡å¼ï¼›ä¸´æ—¶è°ƒè¯•å¯ç”¨ç¯å¢ƒå˜é‡ QUICK_TEST=1
QUICK_TEST_LIMIT_PER_TYPE = 5  # æ¯ç±»(å›¾ç‰‡/Canvas)æœ€å¤šå¤„ç†å¤šå°‘æ¡

def validate_config():
    """éªŒè¯å¿…è¦çš„é…ç½®æ˜¯å¦å­˜åœ¨"""
    if not NOTION_API_KEY:
        print("âŒ é”™è¯¯: è¯·å¡«å†™ NOTION_API_KEY!")
        print("è¯·åœ¨è„šæœ¬é¡¶éƒ¨çš„é…ç½®åŒºåŸŸå¡«å…¥ä½ çš„ Notion API å¯†é’¥")
        print("è·å–æ–¹å¼: https://www.notion.so/my-integrations")
        return False
    
    if not NOTION_DATABASE_ID:
        print("âŒ é”™è¯¯: è¯·å¡«å†™ NOTION_DATABASE_ID!")
        print("è¯·åœ¨è„šæœ¬é¡¶éƒ¨çš„é…ç½®åŒºåŸŸå¡«å…¥ä½ çš„ Notion æ•°æ®åº“ ID")
        print("è·å–æ–¹å¼: ä»æ•°æ®åº“URLä¸­å¤åˆ¶IDéƒ¨åˆ†")
        return False
    
    if len(NOTION_API_KEY) < 10 or not NOTION_API_KEY.startswith(('ntn_', 'secret_')):
        print("âŒ é”™è¯¯: NOTION_API_KEY æ ¼å¼ä¸æ­£ç¡®!")
        print("APIå¯†é’¥åº”è¯¥ä»¥ 'ntn_' æˆ– 'secret_' å¼€å¤´")
        return False
        
    if len(NOTION_DATABASE_ID) != 32:
        print("âŒ é”™è¯¯: NOTION_DATABASE_ID æ ¼å¼ä¸æ­£ç¡®!")
        print("æ•°æ®åº“IDåº”è¯¥æ˜¯32ä½å­—ç¬¦ä¸²")
        return False
    
    return True

def get_database_info(headers, database_id):
    """è·å–æ•°æ®åº“ä¿¡æ¯ï¼Œæ£€æŸ¥å±æ€§ç»“æ„"""
    try:
        response = requests.get(
            f"{NOTION_API_BASE_URL}/databases/{database_id}",
            headers=headers,
            timeout=30
        )
        response.raise_for_status()
        db_info = response.json()
        
        properties = db_info.get('properties', {})
        
        # æŸ¥æ‰¾å„ç§ç±»å‹çš„å±æ€§
        title_property = None
        created_time_property = None
        updated_time_property = None
        conversation_id_property = None
        conversation_id_type = None
        
        for prop_name, prop_info in properties.items():
            prop_type = prop_info.get('type')
            prop_name_lower = prop_name.lower()
            
            if prop_type == 'title':
                title_property = prop_name
            elif prop_type in ['date', 'created_time']:
                if 'created' in prop_name_lower or 'create' in prop_name_lower:
                    created_time_property = prop_name
                elif 'updated' in prop_name_lower or 'update' in prop_name_lower or 'modified' in prop_name_lower:
                    updated_time_property = prop_name
            elif prop_type in ['rich_text', 'number']:
                if ('conversation' in prop_name_lower and 'id' in prop_name_lower) or prop_name_lower == 'conversation id':
                    conversation_id_property = prop_name
                    conversation_id_type = prop_type
        
        return {
            'title_property': title_property or 'Title',
            'created_time_property': created_time_property,
            'updated_time_property': updated_time_property, 
            'conversation_id_property': conversation_id_property,
            'conversation_id_type': conversation_id_type,
            'properties': properties
        }
        
    except requests.exceptions.RequestException as e:
        error_msg = e.response.text if e.response else str(e)
        print(f"âš ï¸ è­¦å‘Š: æ— æ³•è·å–æ•°æ®åº“ä¿¡æ¯: {error_msg}")
        return {
            'title_property': 'Title',
            'created_time_property': None,
            'updated_time_property': None,
            'conversation_id_property': None,
            'properties': {}
        }

# --- å…¨å±€å˜é‡ ---
CONVERSATIONS_JSON_PATH = os.path.join(CHATGPT_EXPORT_PATH, 'conversations.json')
NOTION_API_BASE_URL = "https://api.notion.com/v1"
PROCESSED_LOG_FILE = 'processed_ids.log'
MAX_TEXT_LENGTH = 1000  # Notionæ–‡æœ¬å—æœ€å¤§é•¿åº¦é™åˆ¶ï¼ˆå‡å°‘ä»¥é¿å…400é”™è¯¯ï¼‰
MAX_TRAVERSE_DEPTH = 1000  # é˜²æ­¢æ— é™å¾ªç¯çš„æœ€å¤§éå†æ·±åº¦
DEBUG_FIRST_FAILURE = True  # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºç¬¬ä¸€ä¸ªå¤±è´¥è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯
DEBUG_DETAILED_ERRORS = True  # æ–°å¢ï¼šè¯¦ç»†é”™è¯¯åˆ†æï¼ˆæ­£å¼è¿è¡Œæ—¶å…³é—­ï¼Œè°ƒè¯•æ—¶å¼€å¯ï¼‰

# æ–°å¢ï¼šé”™è¯¯åˆ†æå‡½æ•°
def analyze_request_payload(payload, title=""):
    """åˆ†æè¯·æ±‚è½½è·ï¼Œè¯†åˆ«å¯èƒ½å¯¼è‡´400é”™è¯¯çš„é—®é¢˜"""
    issues = []
    payload_str = json.dumps(payload, ensure_ascii=False)
    
    # æ£€æŸ¥è½½è·å¤§å° - é™ä½é˜ˆå€¼
    size = len(payload_str)
    if size > 400000:  # ä»3000é™ä½åˆ°2000
        issues.append(f"è½½è·è¿‡å¤§: {size} å­—ç¬¦")
    
    # æ£€æŸ¥å¯èƒ½æœ‰é—®é¢˜çš„å†…å®¹æ¨¡å¼
    problematic_patterns = [
        (r'open_url\(', "åŒ…å«open_urlå‡½æ•°è°ƒç”¨"),
        (r'search\(', "åŒ…å«searchå‡½æ•°è°ƒç”¨"),
        (r'https?://[^\s<>"]{50,}', "åŒ…å«è¶…é•¿URL"),
        (r'["\']æˆ‘ä¸çŸ¥é“["\']', "åŒ…å«å¸¦å¼•å·çš„ä¸­æ–‡"),
        (r'["\'][^"\']{100,}["\']', "åŒ…å«è¶…é•¿å¼•å·å­—ç¬¦ä¸²"),
        (r'\\u[0-9a-fA-F]{4}', "åŒ…å«Unicodeè½¬ä¹‰åºåˆ—"),
        (r'\{[^}]{200,}\}', "åŒ…å«è¶…é•¿JSONå¯¹è±¡"),
        (r'Fatal error:|Warning:|Exception:', "åŒ…å«é”™è¯¯æ—¥å¿—"),
        (r'ğŸ‘¤|ğŸ¤–|ğŸ”|ğŸ’¬', "åŒ…å«emojiå­—ç¬¦"),
    ]
    
    for pattern, description in problematic_patterns:
        if re.search(pattern, payload_str):
            matches = len(re.findall(pattern, payload_str))
            issues.append(f"{description} ({matches}å¤„)")
    
    # æ£€æŸ¥åµŒå¥—æ·±åº¦
    if payload_str.count('{') > 20:
        issues.append(f"JSONåµŒå¥—è¿‡æ·±: {payload_str.count('{')} å±‚")
    
    # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
    special_chars = ['"', "'", '\\', '\n', '\t']
    for char in special_chars:
        count = payload_str.count(char)
        if count > 50:
            issues.append(f"ç‰¹æ®Šå­—ç¬¦'{char}'è¿‡å¤š: {count}ä¸ª")
    
    return issues

# æ–°å¢ï¼šå¤±è´¥è½½è·åˆ†æå™¨
def debug_failed_payload(payload, error_response, title):
    """è¯¦ç»†åˆ†æå¤±è´¥çš„è½½è·"""
    if not DEBUG_DETAILED_ERRORS:
        return
    
    print(f"\nğŸ” è¯¦ç»†åˆ†æå¤±è´¥è½½è·: {title}")
    
    # åˆ†æè½½è·é—®é¢˜
    issues = analyze_request_payload(payload, title)
    if issues:
        print("   ğŸš¨ å‘ç°çš„é—®é¢˜:")
        for i, issue in enumerate(issues[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ªé—®é¢˜
            print(f"      {i}. {issue}")
    
    # åˆ†æé”™è¯¯å“åº”
    if error_response:
        try:
            error_detail = error_response.json()
            print("   ğŸ“‹ APIé”™è¯¯è¯¦æƒ…:")
            print(f"      çŠ¶æ€ç : {error_response.status_code}")
            if 'message' in error_detail:
                print(f"      æ¶ˆæ¯: {error_detail['message']}")
            if 'code' in error_detail:
                print(f"      é”™è¯¯ä»£ç : {error_detail['code']}")
        except:
            print(f"   ğŸ“‹ åŸå§‹é”™è¯¯: {error_response.text[:200]}...")
    
    # æå–å¹¶æ˜¾ç¤ºé—®é¢˜å—
    if 'children' in payload:
        print("   ğŸ“¦ é—®é¢˜å—åˆ†æ:")
        for i, block in enumerate(payload['children'][:3], 1):
            block_str = json.dumps(block, ensure_ascii=False)
            block_issues = analyze_request_payload({'block': block})
            print(f"      å—{i} ({len(block_str)}å­—ç¬¦): {', '.join(block_issues) if block_issues else 'æœªå‘ç°é—®é¢˜'}")
    
    print("   " + "="*50)

# --- è¾…åŠ©å‡½æ•° ---
def load_processed_ids():
    """åŠ è½½å·²å¤„ç†çš„å¯¹è¯IDï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ """
    if not os.path.exists(PROCESSED_LOG_FILE):
        return set()
    try:
        with open(PROCESSED_LOG_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {e}")
        return set()

def log_processed_id(conversation_id):
    """è®°å½•æˆåŠŸå¤„ç†çš„å¯¹è¯ID"""
    try:
        with open(PROCESSED_LOG_FILE, 'a', encoding='utf-8') as f:
            f.write(f"{conversation_id}\n")
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶: {e}")

def split_long_text(text, max_length=MAX_TEXT_LENGTH):
    """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆç¬¦åˆNotioné™åˆ¶çš„å—"""
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    current_pos = 0
    
    while current_pos < len(text):
        end_pos = current_pos + max_length
        if end_pos >= len(text):
            chunks.append(text[current_pos:])
            break
        
        # å°è¯•åœ¨å¥å­æˆ–æ®µè½è¾¹ç•Œåˆ†å‰²
        best_split = end_pos
        for i in range(max(current_pos, end_pos - 100), end_pos):
            if text[i] in '.ã€‚\n!ï¼?ï¼Ÿ':
                best_split = i + 1
                break
        
        chunks.append(text[current_pos:best_split])
        current_pos = best_split
    
    return chunks

def upload_file_to_notion(local_file_path, headers):
    """ä¸Šä¼ æ–‡ä»¶åˆ°Notionï¼Œæ”¯æŒå›¾ç‰‡ç­‰é™„ä»¶ (å¢å¼ºå¤šè·¯å¾„æŸ¥æ‰¾)"""

    def find_local_file(path_or_name: str) -> str | None:
        """åœ¨å¸¸è§å­ç›®å½•(images/ dalle-generations/)ä¸­æŸ¥æ‰¾æ–‡ä»¶"""
        if os.path.isabs(path_or_name) and os.path.exists(path_or_name):
            return path_or_name

        # å»é™¤å¯èƒ½çš„å‰ç¼€ "./"ï¼Œå¹¶è§„èŒƒåŒ–è·¯å¾„
        if path_or_name.startswith("./") or path_or_name.startswith(".\\"):
            path_or_name = path_or_name[2:]

        # ç»Ÿä¸€ä½¿ç”¨è§„èŒƒåŒ–åçš„åå­—åšè¿›ä¸€æ­¥å¤„ç†
        abs_path = os.path.join(CHATGPT_EXPORT_PATH, path_or_name)
        if os.path.exists(abs_path):
            return abs_path

        # å¸¸è§å­ç›®å½•
        basename_only = os.path.basename(path_or_name)
        for sub in ["images", "assets", "dalle-generations", "dalle_generations"]:
            candidate = os.path.join(CHATGPT_EXPORT_PATH, sub, basename_only)
            if os.path.exists(candidate):
                return candidate

        # ç¬¬ä¸€è½®ï¼šé’ˆå¯¹ä»¥ file- å¼€å¤´çš„é€šç”¨è§„åˆ™
        if basename_only.startswith("file-"):
            prefix = basename_only.split('.')[0]  # file-XXXXXX
            for root, _dirs, files in os.walk(CHATGPT_EXPORT_PATH):
                for fname in files:
                    if fname.startswith(prefix):
                        return os.path.join(root, fname)

        # ç¬¬äºŒè½®ï¼šæ›´é€šç”¨çš„å‰ç¼€åŒ¹é…ï¼ˆä¸é™å®š file- å‰ç¼€ï¼‰ï¼Œ
        # ä»¥å¤„ç†æ ¹ç›®å½•ä¸‹è¯¸å¦‚ "image-XXX.png" æˆ– "pic_XXX.jpg" ç­‰æƒ…å†µ
        generic_prefix = os.path.splitext(basename_only)[0]
        if len(generic_prefix) > 3:  # é¿å…å‰ç¼€è¿‡çŸ­é€ æˆè¯¯åŒ¹é…
            for root, _dirs, files in os.walk(CHATGPT_EXPORT_PATH):
                for fname in files:
                    if fname.startswith(generic_prefix):
                        return os.path.join(root, fname)

        # ç¬¬ä¸‰è½®ï¼šæ— æ‰©å±•å -> è¯•æ¢å¸¸è§å›¾ç‰‡æ‰©å±•
        if '.' not in basename_only:
            COMMON_EXTS = ['png', 'jpg', 'jpeg', 'webp', 'gif']
            for ext in COMMON_EXTS:
                candidate = os.path.join(CHATGPT_EXPORT_PATH, f"{basename_only}.{ext}")
                if os.path.exists(candidate):
                    return candidate
                # äº¦åœ¨å¸¸è§å­ç›®å½•ä¸­æŸ¥æ‰¾
                for sub in ["images", "assets", "dalle-generations", "dalle_generations"]:
                    candidate_sub = os.path.join(CHATGPT_EXPORT_PATH, sub, f"{basename_only}.{ext}")
                    if os.path.exists(candidate_sub):
                        return candidate_sub
        return None

    actual_path = find_local_file(local_file_path)
    if actual_path is None:
        tqdm.write(f"   âš ï¸ å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {local_file_path}")
        return None

    local_file_path = actual_path

    file_name = os.path.basename(local_file_path)
    file_size = os.path.getsize(local_file_path)
    
    # ====== ä¼šå‘˜ç‰ˆé™åˆ¶ï¼š20 MB ======
    MAX_FILE_SIZE_BYTES = 20 * 1024 * 1024  # 20MB
    if file_size > MAX_FILE_SIZE_BYTES:
        tqdm.write(f"   âš ï¸ æ–‡ä»¶è¿‡å¤§ (>20MB): {local_file_path}")
        return None

    # ====== MIME ç±»å‹åˆ¤å®š ======
    content_type, _ = mimetypes.guess_type(local_file_path)
    # æ‰©å±•ååˆ° MIME çš„è¡¥å……æ˜ å°„
    ext = os.path.splitext(local_file_path)[1].lower().lstrip('.')
    EXT_MIME_MAP = {
        'webp': 'image/webp',
        'heic': 'image/heic',
        'heif': 'image/heic',
        'wav': 'audio/wav',
        'webm': 'video/webm',
    }
    if not content_type and ext in EXT_MIME_MAP:
        content_type = EXT_MIME_MAP[ext]
    if not content_type:
        content_type = 'application/octet-stream'

    # ====== magic bytes æ£€æµ‹ï¼ˆå¤„ç†æ— æ‰©å±•åæ–‡ä»¶å¹¶è¡¥MIMEï¼‰ ======
    if content_type == 'application/octet-stream':
        try:
            with open(local_file_path, 'rb') as fb:
                header = fb.read(20)
            def _match(hdr: bytes, sig: bytes, offset: int = 0):
                return hdr.startswith(sig) if offset == 0 else hdr[offset:offset+len(sig)] == sig

            mime_ext = None
            if _match(header, b'\x89PNG'):
                content_type, mime_ext = 'image/png', 'png'
            elif _match(header, b'\xFF\xD8\xFF'):
                content_type, mime_ext = 'image/jpeg', 'jpg'
            elif header[:6] in (b'GIF87a', b'GIF89a'):
                content_type, mime_ext = 'image/gif', 'gif'
            elif header[:4] == b'RIFF' and b'WEBP' in header[8:16]:
                content_type, mime_ext = 'image/webp', 'webp'
            elif _match(header, b'%PDF'):
                content_type, mime_ext = 'application/pdf', 'pdf'
            elif header[:4] == b'RIFF' and b'WAVE' in header[8:16]:
                content_type, mime_ext = 'audio/wav', 'wav'
            elif header[4:8] == b'ftyp':
                content_type, mime_ext = 'video/mp4', 'mp4'

            # å¦‚æ–‡ä»¶åæ— æ‰©å±•ä¸”è¯†åˆ«æˆåŠŸï¼Œè¡¥ä¸Šæ‰©å±•å (ä»…å½±å“ä¸Šä¼ æ–‡ä»¶åï¼Œä¸æ”¹ç£ç›˜æ–‡ä»¶)
            if mime_ext and '.' not in file_name:
                file_name += f'.{mime_ext}'
        except Exception:
            pass

    # ====== æ”¯æŒçš„ MIME ç™½åå• ======
    ALLOWED_MIME = {
        # å›¾ç‰‡
        'image/jpeg','image/jpg','image/png','image/gif','image/webp','image/svg+xml','image/tiff','image/heic','image/vnd.microsoft.icon',
        # æ–‡æ¡£
        'application/pdf','text/plain','application/json',
        # éŸ³é¢‘
        'audio/mpeg','audio/mp4','audio/aac','audio/midi','audio/ogg','audio/wav','audio/x-ms-wma',
        # è§†é¢‘
        'video/mp4','video/webm','video/quicktime','video/x-msvideo','video/x-flv','video/mpeg','video/x-ms-asf','video/x-amv'
    }
    if content_type not in ALLOWED_MIME:
        tqdm.write(f"   âš ï¸ ä¸æ”¯æŒçš„ MIME ç±»å‹({content_type})ï¼Œè·³è¿‡: {file_name}")
        return None

    # è°ƒè¯•ï¼šæ˜¾ç¤ºæ–‡ä»¶å‡†å¤‡ä¿¡æ¯
    if DEBUG_IMAGE_UPLOAD or os.getenv("DEBUG_IMAGE_UPLOAD") == "1":
        tqdm.write(f"   [DEBUG] å‡†å¤‡ä¸Šä¼ : {file_name} | size={round(file_size/1024,1)}KB | mime={content_type}")

    # ç¬¬ä¸€æ­¥ï¼šå‘Notionè¯·æ±‚ä¸Šä¼ URL
    upload_url = f"{NOTION_API_BASE_URL}/file_uploads"
    payload = {
        "filename": file_name,
        "content_type": content_type
    }
    
    try:
        response = requests.post(upload_url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        upload_data = response.json()
        
        # è°ƒè¯•: è¾“å‡ºä¸Šä¼ è¿”å›ä¿¡æ¯
        if DEBUG_IMAGE_UPLOAD or os.getenv("DEBUG_IMAGE_UPLOAD") == "1":
            tqdm.write(f"   [DEBUG] ä¸Šä¼ è¿”å›: {json.dumps(upload_data, ensure_ascii=False)}")
        
        # ç¬¬äºŒæ­¥ï¼šä¸Šä¼ æ–‡ä»¶å†…å®¹åˆ°è·å–çš„URL
        with open(local_file_path, 'rb') as f:
            file_bytes = f.read()

        base_upload_headers = {
            "Content-Type": content_type,
            "Content-Length": str(file_size)
        }

        upload_url = upload_data["upload_url"]

        # å¦‚æœ upload_url åŒ…å« /sendï¼ŒæŒ‰ Notion API éœ€è¦å¸¦æˆæƒä½¿ç”¨ POST
        if "/send" in upload_url:
            # ä½¿ç”¨ multipart/form-data, requests è‡ªåŠ¨ç”Ÿæˆ boundary ä¸ Content-Type
            upload_headers = {
                "Authorization": headers.get("Authorization", ""),
                "Notion-Version": headers.get("Notion-Version", "2022-06-28")
            }

            files = {
                "file": (file_name, file_bytes, content_type)
            }

            response = requests.post(
                upload_url,
                headers=upload_headers,
                files=files,
                timeout=120
            )
        else:
            # é¢„ç­¾å S3 URLï¼Œä½¿ç”¨ PUT æ— éœ€æˆæƒ
            response = requests.put(
                upload_url,
                headers=base_upload_headers,
                data=file_bytes,
                timeout=120
            )
        response.raise_for_status()
        
        tqdm.write(f"   âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {file_name}")

        if DEBUG_IMAGE_UPLOAD or os.getenv("DEBUG_IMAGE_UPLOAD") == "1":
            tqdm.write(f"   [DEBUG] FileUpload ID: {upload_data.get('id')}")
        return upload_data["id"]
        
    except requests.exceptions.RequestException as e:
        error_msg = e.response.text if e.response else str(e)
        tqdm.write(f"   âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {error_msg}")
        return None

def build_blocks_from_conversation(conversation_data, headers):
    """ä»å¯¹è¯æ•°æ®æ„å»ºNotionå—ï¼Œå¢åŠ äº†å®‰å…¨ä¿æŠ¤"""
    mapping = conversation_data.get('mapping', {})
    if not mapping:
        return []

    # æ‰¾åˆ°æ ¹èŠ‚ç‚¹
    root_id = next((nid for nid, node in mapping.items() if not node.get('parent')), None)
    if not root_id:
        try:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ ¹èŠ‚ç‚¹ï¼Œæ‰¾æœ€æ—©çš„æ¶ˆæ¯ä½œä¸ºèµ·ç‚¹
            root_id = min(mapping.keys(), 
                         key=lambda k: mapping[k].get('message', {}).get('create_time', float('inf')))
        except (ValueError, TypeError):
            return []

    blocks = []
    current_id = root_id
    visited = set()  # é˜²æ­¢æ— é™å¾ªç¯
    depth = 0
    
    # Canvas æ–‡æ¡£å»é‡é›†åˆï¼ˆæŒ‰ textdoc_idï¼‰
    seen_canvas_docs = set()
    
    # å®‰å…¨éå†å¯¹è¯æ ‘
    while current_id in mapping and current_id not in visited and depth < MAX_TRAVERSE_DEPTH:
        visited.add(current_id)
        depth += 1
        
        node = mapping.get(current_id, {})
        message = node.get('message')

        if message and isinstance(message.get('metadata'), dict) and 'canvas' in message['metadata']:
            canvas_meta = message['metadata']['canvas']
            textdoc_id = canvas_meta.get('textdoc_id')
            if textdoc_id and textdoc_id not in seen_canvas_docs:
                seen_canvas_docs.add(textdoc_id)

                canvas_title = canvas_meta.get('title') or canvas_meta.get('textdoc_type', 'Canvas')
                canvas_type = canvas_meta.get('textdoc_type', 'document')
                canvas_version = canvas_meta.get('version')

                desc_lines = [f"Canvas æ¨¡å— -> æ ‡é¢˜: {canvas_title}"]
                desc_lines.append(f"ç±»å‹: {canvas_type} | ç‰ˆæœ¬: {canvas_version} | ID: {textdoc_id}")

                desc_text = "\n".join(filter(None, desc_lines))

                for chunk in split_long_text(desc_text):
                    block = {
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [{"type": "text", "text": {"content": chunk}}]
                        }
                    }
                    validated_block = validate_block_content(block)
                    if validated_block:
                        blocks.append(validated_block)

        if message and message.get('content'):
            author_role = message.get('author', {}).get('role', 'unknown')
            
            # è§’è‰²æ˜ å°„
            speaker_map = {
                "user": "ğŸ‘¤ ç”¨æˆ·",
                "assistant": "ğŸ¤– åŠ©æ‰‹", 
                "tool": f"ğŸ› ï¸ å·¥å…· ({message.get('author', {}).get('name', '')})",
                "system": "âš™ï¸ ç³»ç»Ÿ"
            }
            speaker_raw = speaker_map.get(author_role, "â“ æœªçŸ¥")

            # å°† "ğŸ‘¤ ç”¨æˆ·" å½¢å¼è½¬æ¢ä¸º "[ğŸ‘¤]ç”¨æˆ·:"  å‰ç¼€
            def format_speaker_label(raw: str) -> str:
                if ' ' in raw:
                    emoji_part, name_part = raw.split(' ', 1)
                    return f"[{emoji_part}]{name_part}:"
                # fallback
                return f"[{raw}]:"

            speaker_label = format_speaker_label(speaker_raw)

            content = message['content']
            content_type = content.get('content_type')
            
            # å¤„ç†çº¯æ–‡æœ¬å†…å®¹
            if content_type == 'text' and content.get('parts'):
                full_content = "".join(part for part in content['parts'] if isinstance(part, str))
                if full_content.strip():
                    # å¤„ç†é•¿æ–‡æœ¬åˆ†å‰²
                    text_chunks = split_long_text(f"{speaker_label}\n{full_content}")
                    for chunk in text_chunks:
                        block = {
                            "type": "paragraph",
                            "paragraph": {
                                "rich_text": [{"type": "text", "text": {"content": chunk}}]
                            }
                        }
                        validated_block = validate_block_content(block)
                        if validated_block:
                            blocks.append(validated_block)
            
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
            elif content_type == 'multimodal_text':
                # å…ˆå¤„ç†æ–‡æœ¬éƒ¨åˆ†
                prompt_text = "".join(part for part in content['parts'] if isinstance(part, str))
                if prompt_text.strip():
                    text_chunks = split_long_text(f"{speaker_label}\n{prompt_text}")
                    for chunk in text_chunks:
                        block = {
                            "type": "paragraph",
                            "paragraph": {
                                "rich_text": [{"type": "text", "text": {"content": chunk}}]
                            }
                        }
                        validated_block = validate_block_content(block)
                        if validated_block:
                            blocks.append(validated_block)
                
                # å¤„ç†å›¾ç‰‡éƒ¨åˆ†
                for part in content['parts']:
                    if isinstance(part, dict) and part.get('content_type') == 'image_asset_pointer':
                        asset_pointer = part.get('asset_pointer', '')
                        if asset_pointer.startswith('file-service://'):
                            file_name = asset_pointer.split('/')[-1]
                            if file_name:
                                local_image_path = os.path.join(CHATGPT_EXPORT_PATH, file_name)
                                file_upload_id = upload_file_to_notion(local_image_path, headers)
                                if file_upload_id:
                                    if DEBUG_IMAGE_UPLOAD or os.getenv("DEBUG_IMAGE_UPLOAD") == "1":
                                        tqdm.write(f"   [DEBUG] æ„å»º image block, id={file_upload_id}")
                                    blocks.append({
                                        "type": "image",
                                        "image": {
                                            "type": "file_upload",
                                            "file_upload": {"id": file_upload_id}
                                        }
                                    })

            # å¤„ç†ä»£ç å—
            elif content_type == 'code' and content.get('text'):
                # æ·»åŠ è¯´è¯è€…æ ‡è¯†
                speaker_block = {
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": speaker_label}}]
                    }
                }
                validated_speaker_block = validate_block_content(speaker_block)
                if validated_speaker_block:
                    blocks.append(validated_speaker_block)
                
                # å¤„ç†é•¿ä»£ç åˆ†å‰²
                code_chunks = split_long_text(content['text'])
                for chunk in code_chunks:
                    code_block = {
                        "type": "code",
                        "code": {
                            "rich_text": [{"type": "text", "text": {"content": chunk}}],
                            "language": get_safe_language_type(content.get('language'))
                        }
                    }
                    validated_code_block = validate_block_content(code_block)
                    if validated_code_block:
                        blocks.append(validated_code_block)

            # å¤„ç†ç³»ç»Ÿé”™è¯¯
            elif content_type == 'system_error' and content.get('text'):
                error_text = f"{speaker_label}\nâ—ï¸ ç³»ç»Ÿé”™è¯¯: {content.get('text')}"
                text_chunks = split_long_text(error_text)
                for chunk in text_chunks:
                    error_block = {
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [{"type": "text", "text": {"content": chunk}}]
                        }
                    }
                    validated_error_block = validate_block_content(error_block)
                    if validated_error_block:
                        blocks.append(validated_error_block)

        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        children = node.get('children', [])
        current_id = children[0] if children and isinstance(children, list) else None
    
    # è­¦å‘Šï¼šå¦‚æœè¾¾åˆ°æœ€å¤§æ·±åº¦
    if depth >= MAX_TRAVERSE_DEPTH:
        tqdm.write(f"   âš ï¸ è­¦å‘Š: è¾¾åˆ°æœ€å¤§éå†æ·±åº¦ ({MAX_TRAVERSE_DEPTH})ï¼Œå¯¹è¯å¯èƒ½ä¸å®Œæ•´")
    
    return blocks

def import_conversation_to_notion(title, create_time, update_time, conversation_id, all_blocks, headers, database_id, db_info):
    """å¯¼å…¥å•ä¸ªå¯¹è¯åˆ°Notionæ•°æ®åº“"""
    if not all_blocks:
        tqdm.write(f"   - è·³è¿‡ç©ºå†…å®¹å¯¹è¯: {title}")
        return True

    # é™åˆ¶æ ‡é¢˜é•¿åº¦ï¼Œé¿å…Notion APIé”™è¯¯
    if len(title) > 100:
        title = title[:97] + "..."
    
    # æ¸…ç†æ ‡é¢˜å†…å®¹
    title = clean_text_content(title)

    # éªŒè¯å’Œæ¸…ç†æ‰€æœ‰å—å†…å®¹
    cleaned_blocks = []
    for block in all_blocks:
        validated_block = validate_block_content(block)
        if validated_block:
            # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœå•ä¸ªå—çš„JSONè¡¨ç¤ºå¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²è€Œä¸æ˜¯è·³è¿‡
            block_json_size = len(json.dumps(validated_block, ensure_ascii=False))
            if block_json_size > 1000:  # éœ€è¦è¿›ä¸€æ­¥åˆ†å‰²çš„å—
                tqdm.write(f"   - ğŸ”„ åˆ†å‰²è¿‡å¤§çš„å— ({block_json_size} å­—ç¬¦)")
                
                # è·å–å—çš„æ–‡æœ¬å†…å®¹è¿›è¡Œåˆ†å‰²
                if validated_block['type'] == 'paragraph':
                    original_content = validated_block['paragraph']['rich_text'][0]['text']['content']
                    # å°†å†…å®¹åˆ†å‰²æˆæ›´å°çš„å—
                    smaller_chunks = split_long_text(original_content, max_length=800)
                    for chunk in smaller_chunks:
                        if chunk.strip():
                            smaller_block = {
                                "type": "paragraph",
                                "paragraph": {
                                    "rich_text": [{"type": "text", "text": {"content": chunk}}]
                                }
                            }
                            cleaned_blocks.append(smaller_block)
                elif validated_block['type'] == 'code':
                    original_content = validated_block['code']['rich_text'][0]['text']['content']
                    language = validated_block['code']['language']
                    # å°†ä»£ç åˆ†å‰²æˆæ›´å°çš„å—
                    smaller_chunks = split_long_text(original_content, max_length=800)
                    for chunk in smaller_chunks:
                        if chunk.strip():
                            smaller_block = {
                                "type": "code",
                                "code": {
                                    "rich_text": [{"type": "text", "text": {"content": chunk}}],
                                    "language": language
                                }
                            }
                            cleaned_blocks.append(smaller_block)
                else:
                    # å…¶ä»–ç±»å‹çš„å—ç›´æ¥æ·»åŠ ï¼Œå¦‚å›¾ç‰‡å—
                    cleaned_blocks.append(validated_block)
            else:
                cleaned_blocks.append(validated_block)
    
    if not cleaned_blocks:
        tqdm.write(f"   - è·³è¿‡ç©ºå†…å®¹å¯¹è¯ï¼ˆæ¸…ç†åæ— æœ‰æ•ˆå—ï¼‰: {title}")
        return True

    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ¸…ç†å‰åçš„å—æ•°é‡
    tqdm.write(f"   - è°ƒè¯•: åŸå§‹å—æ•° {len(all_blocks)} -> æ¸…ç†åå—æ•° {len(cleaned_blocks)}")

    # ========== æ–°ç­–ç•¥ï¼šå…ˆåˆ›å»ºç©ºé¡µé¢ï¼Œå†è¿½åŠ æ‰€æœ‰å— ==========
    # é¡µé¢åˆ›å»ºæ—¶ä¸æºå¸¦ childrenï¼Œå¯å¤§å¹…é™ä½ 400 æŠ¥é”™æ¦‚ç‡
    initial_blocks: list = []  # ä¿æŒç©ºåˆ—è¡¨
    remaining_blocks: list = cleaned_blocks  # å…¨é‡å†…å®¹ç¨ååˆ†æ‰¹è¿½åŠ 

    # å°†å‰©ä½™ blocks åˆ†æˆæ›´å°çš„æ‰¹æ¬¡ï¼ˆæœ€å¤š 20 ä¸ª/æ‰¹ï¼‰
    block_chunks = [remaining_blocks[i:i + 20] for i in range(0, len(remaining_blocks), 20)]
    initial_payload_size = 0  # ç©ºè½½è·
    tqdm.write(f"   - åˆ†å—ç­–ç•¥: åˆ›å»ºç©ºé¡µé¢ï¼Œåç»­ {len(block_chunks)} æ‰¹æ¬¡è¿½åŠ ")

    # ä½¿ç”¨æ£€æµ‹åˆ°çš„å±æ€§åç§°
    title_property = db_info.get('title_property', 'Title')
    created_time_property = db_info.get('created_time_property')
    updated_time_property = db_info.get('updated_time_property')
    conversation_id_property = db_info.get('conversation_id_property')
    conversation_id_type = db_info.get('conversation_id_type')

    # åˆ›å»ºé¡µé¢è½½è· - ä½¿ç”¨æ£€æµ‹åˆ°çš„å®Œæ•´å±æ€§ç»“æ„
    properties = {
        title_property: {"title": [{"type": "text", "text": {"content": title}}]}
    }

    # ä»…å½“å±æ€§çœŸæ­£æ˜¯å¯å†™çš„ "date" ç±»å‹æ—¶æ‰å†™å…¥ï¼Œé¿å…ä¿®æ”¹ "created_time" / "last_edited_time" åªè¯»å­—æ®µ
    if created_time_property:
        prop_info = db_info.get('properties', {}).get(created_time_property, {})
        if prop_info.get('type') == 'date':
            properties[created_time_property] = {
                "date": {"start": datetime.datetime.fromtimestamp(create_time).isoformat() + "Z"}
            }
    if updated_time_property:
        prop_info = db_info.get('properties', {}).get(updated_time_property, {})
        if prop_info.get('type') == 'date':
            properties[updated_time_property] = {
                "date": {"start": datetime.datetime.fromtimestamp(update_time).isoformat() + "Z"}
            }

    # æ·»åŠ å¯¹è¯IDå±æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if conversation_id_property:
        if conversation_id_type == 'number':
            try:
                if conversation_id.replace('-', '').isdigit():
                    number_value = int(conversation_id.replace('-', ''))
                else:
                    number_value = abs(hash(conversation_id)) % (10 ** 10)
                properties[conversation_id_property] = {"number": number_value}
            except (ValueError, TypeError):
                number_value = abs(hash(conversation_id)) % (10 ** 10)
                properties[conversation_id_property] = {"number": number_value}
        else:
            properties[conversation_id_property] = {
                "rich_text": [{"type": "text", "text": {"content": conversation_id}}]
            }

    create_payload = {
        "parent": {"database_id": database_id},
        "properties": properties
    }

    # åˆ›å»ºé¡µé¢
    try:
        response = requests.post(
            f"{NOTION_API_BASE_URL}/pages",
            headers=headers,
            data=json.dumps(create_payload),
            timeout=30
        )
        response.raise_for_status()
        page_data = response.json()
        page_id = page_data["id"]
        tqdm.write(f"   - âœ… é¡µé¢åˆ›å»ºæˆåŠŸ: {title}")
    except requests.exceptions.RequestException as e:
        global DEBUG_FIRST_FAILURE
        error_msg = ""
        if e.response:
            try:
                error_detail = e.response.json()
                error_msg = json.dumps(error_detail, indent=2, ensure_ascii=False)
            except:
                error_msg = e.response.text
        else:
            error_msg = str(e)
        
        tqdm.write(f"   - âŒ é¡µé¢åˆ›å»ºå¤±è´¥: {title}")
        tqdm.write(f"   - HTTPçŠ¶æ€ç : {e.response.status_code if e.response else 'N/A'}")
        tqdm.write(f"   - è¯¦ç»†é”™è¯¯: {error_msg}")
        
        # ğŸ¯ æ–°å¢ï¼šä½¿ç”¨æ–°çš„é”™è¯¯åˆ†æå™¨
        debug_failed_payload(create_payload, e.response, title)
        
        # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºç¬¬ä¸€ä¸ªå¤±è´¥è¯·æ±‚çš„å®Œæ•´è½½è·
        if DEBUG_FIRST_FAILURE:
            tqdm.write(f"   - ğŸ› è°ƒè¯•è½½è· (ç¬¬ä¸€æ¬¡å¤±è´¥):")
            tqdm.write(f"     æ ‡é¢˜: {title}")
            tqdm.write(f"     å—æ•°é‡: {len(initial_blocks)}")
            
            # æ˜¾ç¤ºå‰3ä¸ªå—çš„ç»“æ„
            for i, block in enumerate(initial_blocks[:3]):
                tqdm.write(f"     å— {i+1}: {json.dumps(block, ensure_ascii=False, indent=4)}")
            
            if len(initial_blocks) > 3:
                tqdm.write(f"     ... è¿˜æœ‰ {len(initial_blocks)-3} ä¸ªå—")
            
            # æ˜¾ç¤ºå®Œæ•´çš„propertieséƒ¨åˆ†
            tqdm.write(f"     Properties: {json.dumps(properties, ensure_ascii=False, indent=4)}")
            
            DEBUG_FIRST_FAILURE = False  # åªæ˜¾ç¤ºç¬¬ä¸€æ¬¡å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯
        elif len(str(create_payload)) < 2000:  # é¿å…è¾“å‡ºè¿‡é•¿çš„è½½è·
            tqdm.write(f"   - è¯·æ±‚è½½è·: {json.dumps(create_payload, indent=2, ensure_ascii=False)}")
        else:
            tqdm.write(f"   - è½½è·å¤§å°: {len(str(create_payload))} å­—ç¬¦ (è¿‡é•¿ï¼Œå·²çœç•¥)")
            tqdm.write(f"   - å—æ•°é‡: {len(initial_blocks)}")
        
        # å°è¯•åˆ›å»ºç®€åŒ–ç‰ˆæœ¬ï¼ˆåªæœ‰æ ‡é¢˜ï¼Œæ— å†…å®¹å—ï¼‰
        try:
            tqdm.write(f"   - ğŸ”„ å°è¯•åˆ›å»ºç®€åŒ–ç‰ˆæœ¬ï¼ˆä»…æ ‡é¢˜ï¼‰...")
            
            # è¿›ä¸€æ­¥ç®€åŒ–æ ‡é¢˜ï¼Œç§»é™¤å¯èƒ½æœ‰é—®é¢˜çš„å­—ç¬¦
            safe_title = re.sub(r'[^\w\s\-\u4e00-\u9fff]', '', title)  # åªä¿ç•™å­—æ¯æ•°å­—ä¸­æ–‡å’ŒåŸºæœ¬ç¬¦å·
            if len(safe_title.strip()) < 2:
                safe_title = f"å¯¹è¯_{conversation_id[:8]}"  # å¦‚æœæ ‡é¢˜è¢«æ¸…ç†å¾—å¤ªå¹²å‡€ï¼Œä½¿ç”¨å¯¹è¯ID
            
            safe_properties = {
                title_property: {"title": [{"type": "text", "text": {"content": safe_title}}]}
            }
            
            # å°è¯•ä¸æ·»åŠ æ—¶é—´å’Œå¯¹è¯IDï¼Œåªåˆ›å»ºæœ€åŸºæœ¬çš„é¡µé¢
            simple_payload = {
                "parent": {"database_id": database_id},
                "properties": safe_properties
            }
            
            response = requests.post(
                f"{NOTION_API_BASE_URL}/pages",
                headers=headers,
                data=json.dumps(simple_payload),
                timeout=30
            )
            response.raise_for_status()
            page_data = response.json()
            page_id = page_data["id"]
            tqdm.write(f"   - âœ… ç®€åŒ–ç‰ˆæœ¬åˆ›å»ºæˆåŠŸ: {safe_title}")
            
            # ä¹‹åå†å°è¯•æ›´æ–°å±æ€§ï¼ˆåˆ†å¼€è¯·æ±‚é™ä½å¤±è´¥é£é™©ï¼‰
            try:
                time.sleep(0.3)
                update_properties = {}
                
                # é€ä¸ªæ·»åŠ å±æ€§ï¼Œå¤±è´¥äº†ä¹Ÿä¸å½±å“å…¶ä»–çš„
                if created_time_property:
                    try:
                        update_properties[created_time_property] = {
                            "date": {"start": datetime.datetime.fromtimestamp(create_time).isoformat() + "Z"}
                        }
                    except:
                        pass
                
                if conversation_id_property and conversation_id_type == 'number':
                    try:
                        number_value = abs(hash(conversation_id)) % (10**8)  # æ›´å°çš„æ•°å­—
                        update_properties[conversation_id_property] = {"number": number_value}
                    except:
                        pass
                
                if update_properties:
                    requests.patch(
                        f"{NOTION_API_BASE_URL}/pages/{page_id}",
                        headers=headers,
                        data=json.dumps({"properties": update_properties}),
                        timeout=30
                    )
            except:
                pass  # æ›´æ–°å±æ€§å¤±è´¥ä¹Ÿæ²¡å…³ç³»ï¼Œè‡³å°‘é¡µé¢åˆ›å»ºäº†
            
            # å°è¯•æ·»åŠ ä¸€ä¸ªç®€å•çš„è¯´æ˜å—
            try:
                time.sleep(0.3)
                note_block = {
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": "åŸå§‹å†…å®¹å¯¼å…¥æ—¶é‡åˆ°æ ¼å¼é—®é¢˜ï¼Œå·²åˆ›å»ºç©ºç™½é¡µé¢ã€‚"}}]
                    }
                }
                
                requests.patch(
                    f"{NOTION_API_BASE_URL}/blocks/{page_id}/children",
                    headers=headers,
                    data=json.dumps({"children": [note_block]}),
                    timeout=30
                )
                
            except:
                pass  # è¯´æ˜å—å¤±è´¥ä¹Ÿæ²¡å…³ç³»
            
            return True  # ç®€åŒ–ç‰ˆæœ¬ç®—æˆåŠŸ
            
        except requests.exceptions.RequestException as e:
            error_msg = e.response.text if e.response else str(e)
            tqdm.write(f"   - âŒ ç®€åŒ–ç‰ˆæœ¬ä¹Ÿåˆ›å»ºå¤±è´¥: {error_msg}")
            return False

    # å¦‚æœè¿˜æœ‰å‰©ä½™å†…å®¹å—ï¼Œåˆ†æ‰¹è¿½åŠ 
    if block_chunks and any(block_chunks):  # æ£€æŸ¥æ˜¯å¦æœ‰éç©ºçš„å—ç»„
        tqdm.write(f"   - ğŸ’¬ æ£€æµ‹åˆ°é•¿å¯¹è¯ï¼Œæ­£åœ¨è¿½åŠ å‰©ä½™å†…å®¹ ({len(block_chunks)} æ‰¹æ¬¡)...")
        append_url = f"{NOTION_API_BASE_URL}/blocks/{page_id}/children"
        
        for i, chunk in enumerate(block_chunks):
            if not chunk:  # è·³è¿‡ç©ºçš„å—ç»„
                continue
            
            # éªŒè¯æ‰¹æ¬¡å†…å®¹
            validated_chunk = []
            chunk_json_size = 0
            
            for block in chunk:
                # é‡æ–°éªŒè¯æ¯ä¸ªå—
                validated_block = validate_block_content(block)
                if validated_block:
                    block_size = len(json.dumps(validated_block, ensure_ascii=False))
                    
                    # å¦‚æœå•ä¸ªå—å¤ªå¤§ï¼Œåˆ†å‰²å®ƒè€Œä¸æ˜¯è·³è¿‡
                    if block_size > 1000:
                        tqdm.write(f"   -   ...ğŸ”„ åˆ†å‰²è¿‡å¤§å— ({block_size} å­—ç¬¦)")
                        
                        # åˆ†å‰²é€»è¾‘
                        if validated_block['type'] == 'paragraph':
                            original_content = validated_block['paragraph']['rich_text'][0]['text']['content']
                            smaller_chunks = split_long_text(original_content, max_length=600)
                            for small_chunk in smaller_chunks:
                                if small_chunk.strip():
                                    smaller_block = {
                                        "type": "paragraph",
                                        "paragraph": {
                                            "rich_text": [{"type": "text", "text": {"content": small_chunk}}]
                                        }
                                    }
                                    smaller_size = len(json.dumps(smaller_block, ensure_ascii=False))
                                    if (chunk_json_size + smaller_size) <= 50000:  # è¿›ä¸€æ­¥é™ä½
                                        validated_chunk.append(smaller_block)
                                        chunk_json_size += smaller_size
                        elif validated_block['type'] == 'code':
                            original_content = validated_block['code']['rich_text'][0]['text']['content']
                            language = validated_block['code']['language']
                            smaller_chunks = split_long_text(original_content, max_length=600)
                            for small_chunk in smaller_chunks:
                                if small_chunk.strip():
                                    smaller_block = {
                                        "type": "code",
                                        "code": {
                                            "rich_text": [{"type": "text", "text": {"content": small_chunk}}],
                                            "language": language
                                        }
                                    }
                                    smaller_size = len(json.dumps(smaller_block, ensure_ascii=False))
                                    if (chunk_json_size + smaller_size) <= 50000:  # è¿›ä¸€æ­¥é™ä½
                                        validated_chunk.append(smaller_block)
                                        chunk_json_size += smaller_size
                        else:
                            # å…¶ä»–ç±»å‹ï¼ˆå¦‚å›¾ç‰‡ï¼‰ç›´æ¥æ·»åŠ ï¼Œä½†æ£€æŸ¥æ€»å¤§å°
                            if (chunk_json_size + block_size) <= 50000:  # è¿›ä¸€æ­¥é™ä½
                                validated_chunk.append(validated_block)
                                chunk_json_size += block_size
                    else:
                        # å—å¤§å°åˆé€‚ï¼Œæ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºæ‰¹æ¬¡é™åˆ¶
                        if (chunk_json_size + block_size) <= 50000:  # è¿›ä¸€æ­¥é™ä½
                            validated_chunk.append(validated_block)
                            chunk_json_size += block_size
            
            if not validated_chunk:
                tqdm.write(f"   -   ...âš ï¸ æ‰¹æ¬¡ {i+1} æ¸…ç†åä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            try:
                time.sleep(0.5)  # ç¨å¾®å¢åŠ å»¶è¿Ÿ
                payload = {"children": validated_chunk}
                payload_size = len(json.dumps(payload, ensure_ascii=False))
                
                # ğŸ¯ è¿›ä¸€æ­¥é™ä½æ‰¹æ¬¡å¤§å°é™åˆ¶
                if payload_size > 400000:  # ä»2500é™ä½åˆ°1500
                    tqdm.write(f"   -   ...âš ï¸ æ‰¹æ¬¡ {i+1} è½½è·è¿‡å¤§ ({payload_size} å­—ç¬¦)ï¼Œè·³è¿‡")
                    continue
                
                response = requests.patch(
                    append_url,
                    headers=headers,
                    data=json.dumps(payload),
                    timeout=30
                )
                response.raise_for_status()
                tqdm.write(f"   -   ...è¿½åŠ æ‰¹æ¬¡ {i+1}/{len(block_chunks)} æˆåŠŸ ({len(validated_chunk)} ä¸ªå—, {payload_size} å­—ç¬¦)")
            except requests.exceptions.RequestException as e:
                error_msg = e.response.text if e.response else str(e)
                tqdm.write(f"   -   ...âŒ è¿½åŠ æ‰¹æ¬¡ {i+1}/{len(block_chunks)} å¤±è´¥: {error_msg}")
                
                # ğŸ¯ æ–°å¢ï¼šåˆ†æè¿½åŠ å¤±è´¥çš„åŸå› 
                debug_failed_payload(payload, e.response, f"{title} - æ‰¹æ¬¡{i+1}")
                
                # ========== æ–°å¢å›é€€ï¼šé€å—å°è¯•æ’å…¥ï¼Œä¿ç•™èƒ½æˆåŠŸçš„ ==========
                tqdm.write(f"   -   ...âš™ï¸ å›é€€åˆ°å•å—è¿½åŠ æ¨¡å¼ï¼Œé€å—é‡è¯•")
                successful_blocks = 0
                for k, single_block in enumerate(validated_chunk):
                    single_payload = {"children": [single_block]}
                    try:
                        time.sleep(0.4)
                        requests.patch(
                            append_url,
                            headers=headers,
                            data=json.dumps(single_payload),
                            timeout=30
                        ).raise_for_status()
                        successful_blocks += 1
                    except requests.exceptions.RequestException:
                        # âš ï¸ å¦‚æœå•å—ä»ç„¶å¤±è´¥ï¼Œå°è¯•å°†å…¶å†æ¬¡åˆ†å‰²ä¸ºæ›´å°æ–‡æœ¬ï¼ˆ300å­—ï¼‰
                        # ä»…å¤„ç† paragraph / code
                        try:
                            if single_block.get('type') in ('paragraph', 'code'):
                                txt_key = 'paragraph' if single_block['type'] == 'paragraph' else 'code'
                                original_txt = single_block[txt_key]['rich_text'][0]['text']['content']
                                tiny_chunks = split_long_text(original_txt, max_length=300)
                                tiny_success = 0
                                for tiny in tiny_chunks:
                                    tiny_block = {
                                        "type": single_block['type'],
                                        txt_key: {
                                            "rich_text": [{"type": "text", "text": {"content": tiny}}]
                                        }
                                    }
                                    try:
                                        time.sleep(0.2)
                                        requests.patch(
                                            append_url,
                                            headers=headers,
                                            data=json.dumps({"children": [tiny_block]}),
                                            timeout=30
                                        ).raise_for_status()
                                        tiny_success += 1
                                    except requests.exceptions.RequestException:
                                        # å¦‚æœæœ€å°å—è¿˜å¤±è´¥ï¼Œå°±å½»åº•æ”¾å¼ƒ
                                        pass
                                if tiny_success:
                                    successful_blocks += tiny_success  # ç»Ÿè®¡æˆåŠŸæ•°
                        except Exception:
                            pass
                        continue
                tqdm.write(f"   -   ...å•å—è¿½åŠ å®Œæˆï¼ŒæˆåŠŸ {successful_blocks}/{len(validated_chunk)} å—")
                # ä¸å› å•æ‰¹å¤±è´¥è€Œåœæ­¢æ•´ä½“æµç¨‹
                continue

    return True

def clean_text_content(text):
    """æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´APIé”™è¯¯çš„å­—ç¬¦"""
    if not isinstance(text, str):
        return str(text)
    
    # ç§»é™¤æ¿€è¿›ç®€åŒ–ï¼šä¸å†å› å…³é”®å­—æˆ–é•¿åº¦ç›´æ¥è¿”å›å ä½æ–‡æœ¬ï¼Œè€Œæ˜¯å°è¯•å®Œæ•´ä¿ç•™å†…å®¹ï¼Œåç»­è‹¥å›  Notion é™åˆ¶å¤±è´¥ï¼Œ
    # å°†ç”±ä¸Šå±‚é€»è¾‘ï¼ˆé€å—æ‹†åˆ† / code block / txt é™„ä»¶ï¼‰å…œåº•å¤„ç†ã€‚
    # ä¿ç•™è¯¥ if ä»…ç”¨äºæ ‡è®°ï¼Œå¯é’ˆå¯¹æç«¯é•¿æ–‡æœ¬è¿›è¡ŒåŸºç¡€æˆªæ–­ï¼Œä½†é»˜è®¤ä¿ç•™å…¨éƒ¨ã€‚
    # ï¼ˆå¦‚ç¡®æœ‰éœ€è¦ï¼Œå¯åœ¨æ­¤å¤„å¯ç”¨ soft_truncate é€»è¾‘ã€‚ï¼‰
    # if len(text) > VERY_LONG_LIMIT: text = text[:VERY_LONG_LIMIT] + "..."
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œã€åˆ¶è¡¨ç¬¦å’Œå›è½¦ï¼‰
    cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
    
    # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
    cleaned = cleaned.replace('\r\n', '\n').replace('\r', '\n')
    
    # å¤„ç†å¯èƒ½æœ‰é—®é¢˜çš„URLå’Œç‰¹æ®Šå­—ç¬¦
    # æ›¿æ¢å¯èƒ½å¯¼è‡´APIé—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
    cleaned = cleaned.replace('\u2028', '\n').replace('\u2029', '\n\n')  # è¡Œåˆ†éš”ç¬¦å’Œæ®µè½åˆ†éš”ç¬¦
    
    # æ–°å¢ï¼šæ¸…ç†PHPé”™è¯¯æ—¥å¿—å’ŒæŠ€æœ¯é”™è¯¯ä¿¡æ¯
    # ç§»é™¤PHP Fatal errorå’ŒWarningä¿¡æ¯
    if 'PHP Fatal error:' in cleaned or 'PHP Warning:' in cleaned or 'PHP Notice:' in cleaned:
        lines = cleaned.split('\n')
        cleaned_lines = []
        skip_next = False
        
        for line in lines:
            # è·³è¿‡PHPé”™è¯¯è¡Œ
            if any(error_type in line for error_type in ['PHP Fatal error:', 'PHP Warning:', 'PHP Notice:', 'Stack trace:', 'thrown in']):
                skip_next = True
                continue
            # è·³è¿‡é”™è¯¯å †æ ˆçš„åç»­è¡Œ
            elif skip_next and (line.startswith('#') or line.startswith('  ')):
                continue
            else:
                skip_next = False
                cleaned_lines.append(line)
        
        cleaned = '\n'.join(cleaned_lines)
    
    # æ–°å¢ï¼šæ¸…ç†WordPress HTMLå†…å®¹
    # ç§»é™¤WordPresså—æ³¨é‡Š
    cleaned = re.sub(r'<!-- wp:[^>]+ -->', '', cleaned)
    cleaned = re.sub(r'<!-- /wp:[^>]+ -->', '', cleaned)
    
    # æ¸…ç†HTMLæ ‡ç­¾ä¸­çš„å¤æ‚å±æ€§
    cleaned = re.sub(r'<([a-zA-Z]+)[^>]*class="[^"]*"[^>]*>', r'<\1>', cleaned)
    cleaned = re.sub(r'<([a-zA-Z]+)[^>]*>', r'<\1>', cleaned)
    
    # æ–°å¢ï¼šå¤„ç†æ–‡ä»¶è·¯å¾„ä¿¡æ¯
    # ç§»é™¤Linux/Unixæ–‡ä»¶è·¯å¾„
    cleaned = re.sub(r'/[a-zA-Z0-9_/.-]+\.php', '[è·¯å¾„å·²æ¸…ç†]', cleaned)
    cleaned = re.sub(r'/home/[a-zA-Z0-9_/.-]+', '[ç›®å½•å·²æ¸…ç†]', cleaned)
    
    # æ–°å¢ï¼šæ¸…ç†è¿‡é•¿çš„URL
    # å°†è¶…é•¿URLæ›¿æ¢ä¸ºç®€åŒ–ç‰ˆæœ¬
    def replace_long_url(match):
        url = match.group(0)
        if len(url) > 100:
            return url[:50] + '...[URLå·²æˆªæ–­]'
        return url
    
    cleaned = re.sub(r'https?://[^\s<>"]+', replace_long_url, cleaned)
    
    # æ–°å¢ï¼šç§»é™¤è¿‡å¤šçš„é‡å¤å­—ç¬¦
    # ç§»é™¤è¿‡å¤šè¿ç»­çš„ç›¸åŒå­—ç¬¦ï¼ˆå¯èƒ½æ˜¯é”™è¯¯è¾“å‡ºï¼‰
    cleaned = re.sub(r'(.)\1{10,}', r'\1\1\1[é‡å¤å†…å®¹å·²æ¸…ç†]', cleaned)
    
    # æ–°å¢ï¼šæ¸…ç†æœç´¢ç»“æœå†…å®¹
    # ç§»é™¤ChatGPTæœç´¢ç»“æœçš„ç‰¹æ®Šæ ¼å¼ # [0]Title - Website [url]
    cleaned = re.sub(r'# \[\d+\].*?\n', '', cleaned)
    
    # æ¸…ç†metadata_listç»“æ„ï¼ˆæœç´¢ç»“æœçš„é‡å¤å…ƒæ•°æ®ï¼‰
    if '"metadata_list":' in cleaned and cleaned.count('"title":') > 5:
        # å¦‚æœåŒ…å«å¤ªå¤šé‡å¤çš„æœç´¢ç»“æœå…ƒæ•°æ®ï¼Œè¿›è¡Œç®€åŒ–
        lines = cleaned.split('\n')
        cleaned_lines = []
        in_metadata = False
        
        for line in lines:
            if '"metadata_list":' in line:
                in_metadata = True
                cleaned_lines.append('æœç´¢ç»“æœå…ƒæ•°æ®å·²ç®€åŒ–...')
                continue
            elif in_metadata and (line.strip().startswith('}') or line.strip() == ']'):
                in_metadata = False
                continue
            elif not in_metadata:
                cleaned_lines.append(line)
        
        cleaned = '\n'.join(cleaned_lines)
    
    # æ–°å¢ï¼šæ¸…ç†æœç´¢ç»“æœçš„é‡å¤å†…å®¹
    # ç§»é™¤Visibleå­—æ®µåçš„é‡å¤æœç´¢ç»“æœ
    if 'Visible' in cleaned:
        parts = cleaned.split('Visible')
        if len(parts) > 1:
            # ä¿ç•™ç¬¬ä¸€éƒ¨åˆ†ï¼Œåé¢çš„é‡å¤æœç´¢ç»“æœç®€åŒ–
            cleaned = parts[0] + '\n[é‡å¤æœç´¢ç»“æœå·²æ¸…ç†]'
    
    # æ–°å¢ï¼šæ¸…ç†Unicodeè½¬ä¹‰åºåˆ—
    # ç§»é™¤\uå½¢å¼çš„Unicodeè½¬ä¹‰åºåˆ—ï¼ˆå¦‚æœè¿‡å¤šï¼‰
    unicode_count = len(re.findall(r'\\u[0-9a-fA-F]{4}', cleaned))
    if unicode_count > 10:  # å¦‚æœUnicodeè½¬ä¹‰å¤ªå¤šï¼Œè¯´æ˜å¯èƒ½æ˜¯æŠ€æœ¯é”™è¯¯ä¿¡æ¯
        cleaned = re.sub(r'\\u[0-9a-fA-F]{4}', '[Unicodeå·²æ¸…ç†]', cleaned)
    
    # æ–°å¢ï¼šæ¸…ç†ç‰¹æ®Šçš„æœç´¢ç»“æœåˆ†éš”ç¬¦
    cleaned = re.sub(r'\u2020+', '|', cleaned)  # æ›¿æ¢â€ ç¬¦å·
    cleaned = re.sub(r'\u2019', "'", cleaned)   # æ›¿æ¢ç‰¹æ®Šå¼•å·
    cleaned = re.sub(r'\u201c|\u201d', '"', cleaned)  # æ›¿æ¢ç‰¹æ®ŠåŒå¼•å·
    
    # ğŸ¯ å¤„ç†emojiå’Œç‰¹æ®Šå­—ç¬¦ï¼šä¿ç•™å¸¸è§èŠå¤©è§’è‰²emojiï¼ˆğŸ‘¤ ğŸ¤– ğŸ› ï¸ï¼‰ï¼Œä»…å¯¹ Notion å¯èƒ½æ‹’ç»çš„ç½•è§ emoji åšæ›¿æ¢
    emoji_replacements = {
        'ğŸ”': '[æœç´¢]',
        'ğŸ’¬': '[å¯¹è¯]',
        'ğŸ“': '[ç¬”è®°]'
    }
    for em, repl in emoji_replacements.items():
        cleaned = cleaned.replace(em, repl)
    
    # å¤„ç†å¯èƒ½æœ‰é—®é¢˜çš„æ ‡ç‚¹ç»„åˆ
    cleaned = cleaned.replace('ï¼š', ':')  # ä¸­æ–‡å†’å·è½¬è‹±æ–‡å†’å·
    cleaned = cleaned.replace('ã€‚"', '.')  # å¥å·+å¼•å·çš„ç»„åˆ
    cleaned = cleaned.replace('"ã€‚', '.')  # å¼•å·+å¥å·çš„ç»„åˆ
    
    # æ–°å¢ï¼šæ¸…ç†è¿‡é•¿çš„æŠ€æœ¯é”™è¯¯ä¿¡æ¯è¡Œ
    lines = cleaned.split('\n')
    cleaned_lines = []
    
    for line in lines:
        # å¦‚æœè¡Œå¤ªé•¿ä¸”åŒ…å«æŠ€æœ¯å…³é”®è¯ï¼Œè¿›è¡Œæˆªæ–­
        if len(line) > 200 and any(keyword in line.lower() for keyword in [
            'error', 'warning', 'exception', 'failed', 'uncaught', 'require', 'include'
        ]):
            cleaned_lines.append(line[:100] + '...[é”™è¯¯ä¿¡æ¯å·²æˆªæ–­]')
        else:
            cleaned_lines.append(line)
    
    cleaned = '\n'.join(cleaned_lines)
    
    # ç§»é™¤è¿‡å¤šçš„è¿ç»­ç©ºç™½å­—ç¬¦
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)  # æœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­æ¢è¡Œ
    cleaned = re.sub(r' {3,}', '  ', cleaned)      # æœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­ç©ºæ ¼
    
    # é™åˆ¶æ–‡æœ¬é•¿åº¦
    if len(cleaned) > MAX_TEXT_LENGTH:
        cleaned = cleaned[:MAX_TEXT_LENGTH-3] + "..."
    
    return cleaned.strip()

def get_safe_language_type(language):
    """è·å–å®‰å…¨çš„ä»£ç è¯­è¨€ç±»å‹ï¼Œç¡®ä¿Notion APIæ”¯æŒ"""
    if not language or language == 'unknown':
        return 'text'
    
    # Notionæ”¯æŒçš„ä¸»è¦è¯­è¨€ç±»å‹
    supported_languages = {
        'javascript', 'typescript', 'python', 'java', 'c', 'cpp', 'c++', 'c#', 'csharp',
        'php', 'ruby', 'go', 'rust', 'swift', 'kotlin', 'scala', 'r', 'matlab',
        'sql', 'html', 'css', 'scss', 'sass', 'xml', 'json', 'yaml', 'yml',
        'markdown', 'bash', 'shell', 'powershell', 'dockerfile', 'makefile',
        'text', 'plain_text', 'plaintext'
    }
    
    language_lower = language.lower().strip()
    
    # ç›´æ¥åŒ¹é…
    if language_lower in supported_languages:
        return language_lower
    
    # å¸¸è§åˆ«åæ˜ å°„
    language_mappings = {
        'js': 'javascript',
        'ts': 'typescript',
        'py': 'python',
        'rb': 'ruby',
        'sh': 'bash',
        'ps1': 'powershell',
        'cs': 'csharp',
        'htm': 'html',
        'jsonl': 'json',
        'yml': 'yaml',
        'md': 'markdown',
        'txt': 'text',
        'c++': 'cpp',
        'objective-c': 'c',
        'objc': 'c'
    }
    
    if language_lower in language_mappings:
        return language_mappings[language_lower]
    
    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›text
    return 'text'

def validate_block_content(block):
    """éªŒè¯å¹¶æ¸…ç†å—å†…å®¹"""
    if not isinstance(block, dict):
        return None
    
    try:
        # éªŒè¯åŸºæœ¬ç»“æ„
        if 'type' not in block:
            return None
        
        block_type = block['type']
        
        # å¤„ç†æ®µè½å—
        if block_type == 'paragraph' and 'paragraph' in block:
            paragraph = block['paragraph']
            if 'rich_text' in paragraph:
                cleaned_rich_text = []
                for text_obj in paragraph['rich_text']:
                    if isinstance(text_obj, dict) and 'text' in text_obj and 'content' in text_obj['text']:
                        content = clean_text_content(text_obj['text']['content'])
                        
                        # ğŸ¯ æ–°å¢ï¼šé’ˆå¯¹ç‰¹æ®Šå†…å®¹çš„æ¿€è¿›æ¸…ç†
                        if any(pattern in content for pattern in ['[å‡½æ•°è°ƒç”¨å·²æ¸…ç†]', 'open_url', 'search(', '1q43.blog']):
                            content = "å†…å®¹åŒ…å«å¯èƒ½å¯¼è‡´APIé”™è¯¯çš„ç‰¹æ®Šå­—ç¬¦ï¼Œå·²è¿›è¡Œç®€åŒ–å¤„ç†ã€‚"
                        
                        if content.strip():  # åªä¿ç•™éç©ºå†…å®¹
                            cleaned_rich_text.append({
                                "type": "text",
                                "text": {"content": content}
                            })
                
                if cleaned_rich_text:
                    return {
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": cleaned_rich_text
                        }
                    }
        
        # å¤„ç†ä»£ç å—
        elif block_type == 'code' and 'code' in block:
            code = block['code']
            if 'rich_text' in code:
                cleaned_rich_text = []
                for text_obj in code['rich_text']:
                    if isinstance(text_obj, dict) and 'text' in text_obj and 'content' in text_obj['text']:
                        content = clean_text_content(text_obj['text']['content'])
                        
                        # ğŸ¯ æ–°å¢ï¼šä»£ç å—çš„æ¿€è¿›æ¸…ç†
                        if any(pattern in content for pattern in ['[å‡½æ•°è°ƒç”¨å·²æ¸…ç†]', 'open_url', 'search(', '1q43.blog']):
                            content = "# ä»£ç å†…å®¹åŒ…å«å‡½æ•°è°ƒç”¨ï¼Œå·²ç®€åŒ–\n# åŸå§‹ä»£ç å¯èƒ½åŒ…å«APIè°ƒç”¨ç­‰å¤æ‚å†…å®¹"
                        
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœä»£ç å—åŒ…å«å¯èƒ½æœ‰é—®é¢˜çš„URLï¼Œè¿›è¡Œé¢å¤–æ¸…ç†
                        elif any(domain in content for domain in ['1q43.blog', 'github.com', 'docs.']) and len(content) > 200:
                            # å°†å¤æ‚çš„ä»£ç å—ç®€åŒ–ä¸ºæ³¨é‡Š
                            content = f"# ä»£ç å†…å®¹åŒ…å«å¤æ‚URLï¼Œå·²ç®€åŒ–\n# åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦"
                        
                        if content.strip():  # åªä¿ç•™éç©ºå†…å®¹
                            cleaned_rich_text.append({
                                "type": "text", 
                                "text": {"content": content}
                            })
                
                if cleaned_rich_text:
                    # ç¡®ä¿è¯­è¨€ç±»å‹å®‰å…¨ï¼Œå¯¹å¯èƒ½æœ‰é—®é¢˜çš„ä»£ç å—å¼ºåˆ¶ä½¿ç”¨text
                    original_language = code.get('language', 'text')
                    language = get_safe_language_type(original_language)
                    
                    # å¦‚æœåŸå§‹å†…å®¹å¯èƒ½æœ‰é—®é¢˜ï¼Œå¼ºåˆ¶ä½¿ç”¨textè¯­è¨€
                    content_text = cleaned_rich_text[0]['text']['content'] if cleaned_rich_text else ''
                    if any(keyword in content_text for keyword in ['å‡½æ•°è°ƒç”¨', 'open_url', 'search(', '# [', '1q43.blog']):
                        language = 'text'
                    
                    return {
                        "type": "code",
                        "code": {
                            "rich_text": cleaned_rich_text,
                            "language": language
                        }
                    }
        
        # å¤„ç†å›¾ç‰‡å—
        elif block_type == 'image':
            return block
        
        return None
        
    except Exception as e:
        print(f"   âš ï¸ è­¦å‘Š: æ¸…ç†å—å†…å®¹æ—¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ ChatGPT åˆ° Notion å¯¼å…¥å™¨")
    
    # éªŒè¯é…ç½®
    if not validate_config():
        print("\nğŸ’¡ æç¤º: è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è®¾ç½®:")
        print("1. è·å–Notion APIå¯†é’¥: https://www.notion.so/my-integrations")
        print("2. è·å–æ•°æ®åº“ID: ä»æ•°æ®åº“URLä¸­å¤åˆ¶")
        print("3. åœ¨è„šæœ¬é¡¶éƒ¨çš„é…ç½®åŒºåŸŸå¡«å…¥è¿™äº›ä¿¡æ¯")
        sys.exit(1)
    
    # è®¾ç½®APIè¯·æ±‚å¤´
    headers = {
        "Authorization": f"Bearer {NOTION_API_KEY}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    # æ£€æµ‹æ•°æ®åº“ç»“æ„
    print("ğŸ” æ£€æµ‹æ•°æ®åº“ç»“æ„...")
    db_info = get_database_info(headers, NOTION_DATABASE_ID)
    
    if not db_info['properties']:
        print("âš ï¸ è­¦å‘Š: æ— æ³•è·å–æ•°æ®åº“å±æ€§ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯æƒé™é—®é¢˜")
        print("è¯·ç¡®ä¿:")
        print("1. ä½ çš„é›†æˆæœ‰è¯¥æ•°æ®åº“çš„è®¿é—®æƒé™")  
        print("2. æ•°æ®åº“è‡³å°‘æœ‰ä¸€ä¸ª'æ ‡é¢˜'ç±»å‹çš„å±æ€§")
    else:
        print(f"âœ… æ•°æ®åº“æ£€æµ‹ç»“æœ:")
        print(f"   ğŸ“ æ ‡é¢˜å±æ€§: {db_info['title_property']}")
        if db_info['created_time_property']:
            print(f"   ğŸ“… åˆ›å»ºæ—¶é—´å±æ€§: {db_info['created_time_property']}")
        if db_info['updated_time_property']:
            print(f"   ğŸ”„ æ›´æ–°æ—¶é—´å±æ€§: {db_info['updated_time_property']}")
        if db_info['conversation_id_property']:
            print(f"   ğŸ†” å¯¹è¯IDå±æ€§: {db_info['conversation_id_property']} ({db_info['conversation_id_type']}ç±»å‹)")
        print(f"   ğŸ“Š æ€»å…±å‘ç° {len(db_info['properties'])} ä¸ªå±æ€§")
    
    # éªŒè¯å¯¹è¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(CONVERSATIONS_JSON_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å¯¹è¯æ–‡ä»¶ '{CONVERSATIONS_JSON_PATH}'")
        print(f"è¯·æ£€æŸ¥ CHATGPT_EXPORT_PATH è®¾ç½®: {CHATGPT_EXPORT_PATH}")
        print("ç¡®ä¿conversations.jsonæ–‡ä»¶åœ¨æŒ‡å®šç›®å½•ä¸­")
        sys.exit(1)

    # è¯»å–å¯¹è¯æ•°æ®
    try:
        with open(CONVERSATIONS_JSON_PATH, 'r', encoding='utf-8') as f:
            all_conversations = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å–å¯¹è¯æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•è¯»å– conversations.json: {e}")
        sys.exit(1)

    # ====== å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä»…æŒ‘é€‰åŒ…å«å›¾ç‰‡æˆ– Canvas çš„å¯¹è¯ ======
    if QUICK_TEST_MODE:
        print("ğŸš§ QUICK_TEST æ¨¡å¼å·²å¯ç”¨ï¼šä»…å¯¼å…¥åŒ…å«å›¾ç‰‡æˆ– Canvas çš„å¯¹è¯â€¦")

        image_convs, canvas_convs = [], []

        def inspect_conversation(conv):
            has_image, has_canvas = False, False
            mapping = conv.get('mapping', {}) or {}
            for node in mapping.values():
                msg = node.get('message') or {}
                # å›¾ç‰‡æ£€æµ‹
                content = msg.get('content') or {}
                if content.get('content_type') == 'multimodal_text':
                    for part in content.get('parts', []):
                        if isinstance(part, dict) and part.get('content_type') == 'image_asset_pointer':
                            has_image = True
                            break
                # Canvas æ£€æµ‹
                if isinstance(msg.get('metadata'), dict) and 'canvas' in msg['metadata']:
                    has_canvas = True
            return has_image, has_canvas

        for conv in all_conversations:
            img, cvs = inspect_conversation(conv)
            if img and len(image_convs) < QUICK_TEST_LIMIT_PER_TYPE:
                image_convs.append(conv)
            if cvs and len(canvas_convs) < QUICK_TEST_LIMIT_PER_TYPE:
                canvas_convs.append(conv)
            # é€€å‡ºæ—©ï¼ŒèŠ‚çœæ—¶é—´
            if len(image_convs) >= QUICK_TEST_LIMIT_PER_TYPE and len(canvas_convs) >= QUICK_TEST_LIMIT_PER_TYPE:
                break

        # åˆå¹¶å¹¶å»é‡
        quick_list = {conv['id']: conv for conv in (image_convs + canvas_convs)}.values()
        conversations_to_process = list(quick_list)
        print(f"ğŸ” QUICK_TEST é€‰ä¸­å¯¹è¯æ•°: {len(conversations_to_process)} (å›¾ç‰‡ {len(image_convs)}, Canvas {len(canvas_convs)})")

    # åŠ è½½å·²å¤„ç†çš„å¯¹è¯ID
    processed_ids = load_processed_ids()

    if QUICK_TEST_MODE:
        # conversations_to_process å·²åœ¨ QUICK_TEST é€»è¾‘ä¸­ç”Ÿæˆï¼Œè¿™é‡Œä»…è¿‡æ»¤å·²å¤„ç†è¿‡çš„
        conversations_to_process = [
            conv for conv in conversations_to_process  # type: ignore  # å·²å®šä¹‰äº QUICK_TEST
            if conv.get('id') not in processed_ids
        ]
    else:
        conversations_to_process = [
            conv for conv in all_conversations 
            if conv.get('id') not in processed_ids and 'title' in conv and 'mapping' in conv
        ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_all = len(all_conversations)
    total_to_process = len(conversations_to_process)
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»å¯¹è¯æ•°: {total_all}")
    print(f"   å·²å¤„ç†: {len(processed_ids)} (å°†è·³è¿‡)")
    print(f"   å¾…å¤„ç†: {total_to_process}")

    if total_to_process == 0:
        print("âœ… æ‰€æœ‰å¯¹è¯å·²å¤„ç†å®Œæˆï¼Œæ— éœ€æ‰§è¡Œ")
        return

    print(f"\nâ–¶ï¸ å¼€å§‹å¤„ç† {total_to_process} ä¸ªæ–°å¯¹è¯...")
    success_count, fail_count = 0, 0
    
    # æŒ‰æ—¶é—´å€’åºå¤„ç†ï¼Œæœ€æ–°çš„å¯¹è¯ä¼˜å…ˆå¯¼å…¥
    for conversation in tqdm(reversed(conversations_to_process), 
                           total=total_to_process, 
                           desc="å¯¼å…¥è¿›åº¦", 
                           unit="å¯¹è¯"):
        conv_id = conversation['id']
        conv_title = conversation.get('title', 'Untitled')
        
        try:
            # æ„å»ºNotionå—
            blocks = build_blocks_from_conversation(conversation, headers)
            
            # å¯¼å…¥åˆ°Notion
            success = import_conversation_to_notion(
                title=conv_title,
                create_time=conversation.get('create_time', time.time()),
                update_time=conversation.get('update_time', time.time()),
                conversation_id=conv_id,
                all_blocks=blocks,
                headers=headers,
                database_id=NOTION_DATABASE_ID,
                db_info=db_info
            )

            if success:
                success_count += 1
                log_processed_id(conv_id)  # åªæœ‰æˆåŠŸæ‰è®°å½•
            else:
                fail_count += 1
                tqdm.write(f"âŒ å¯¼å…¥å¤±è´¥: '{conv_title}' (ä¸‹æ¬¡è¿è¡Œæ—¶å°†é‡è¯•)")

        except Exception as e:
            fail_count += 1
            tqdm.write(f"âŒ å¤„ç† '{conv_title}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

        # é¿å…APIé€Ÿç‡é™åˆ¶
        time.sleep(0.4)

    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n" + "="*50)
    print("ğŸ‰ å¯¼å…¥å®Œæˆ! ç»“æœç»Ÿè®¡:")
    print(f"ğŸŸ¢ æˆåŠŸå¯¼å…¥: {success_count} ä¸ªå¯¹è¯")
    if fail_count > 0:
        print(f"ğŸ”´ å¯¼å…¥å¤±è´¥: {fail_count} ä¸ªå¯¹è¯")
        print("   ğŸ’¡ å¤±è´¥çš„å¯¹è¯å°†åœ¨ä¸‹æ¬¡è¿è¡Œæ—¶é‡è¯•")
    print(f"â­ï¸  è·³è¿‡ (å·²å¤„ç†): {len(processed_ids)} ä¸ªå¯¹è¯")
    
    if success_count > 0:
        print(f"\nâœ¨ è¯·åˆ°ä½ çš„Notionæ•°æ®åº“æŸ¥çœ‹å¯¼å…¥çš„ {success_count} ä¸ªå¯¹è¯!")

if __name__ == "__main__":
    main() 
